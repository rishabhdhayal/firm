{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c94b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth Table of OR\n",
      "   input1  input2  actual  predicted\n",
      "0       0       0       0          1\n",
      "1       0       1       1          1\n",
      "2       1       0       1          1\n",
      "3       1       1       1          1\n",
      "\n",
      "Truth Table of AND\n",
      "   input1  input2  actual  predicted\n",
      "0       0       0       0          1\n",
      "1       0       1       0          1\n",
      "2       1       0       0          1\n",
      "3       1       1       1          1\n",
      "\n",
      "Truth Table of XOR\n",
      "   input1  input2  actual  predicted\n",
      "0       0       0       0          1\n",
      "1       0       1       1          1\n",
      "2       1       0       1          1\n",
      "3       1       1       0          1\n",
      "\n",
      "Truth Table of NOR\n",
      "   input1  input2  actual  predicted\n",
      "0       0       0       1          1\n",
      "1       0       1       0          1\n",
      "2       1       0       0          1\n",
      "3       1       1       0          1\n",
      "\n",
      "Truth Table of NAND\n",
      "   input1  input2  actual  predicted\n",
      "0       0       0       1          1\n",
      "1       0       1       1          1\n",
      "2       1       0       1          1\n",
      "3       1       1       0          1\n",
      "\n",
      "Truth Table of XNOR\n",
      "   input1  input2  actual  predicted\n",
      "0       0       0       1          1\n",
      "1       0       1       0          1\n",
      "2       1       0       0          1\n",
      "3       1       1       1          1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question 4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def sigmoid(x):\n",
    " return 1 / (1 + np.exp(-x))\n",
    "def neural_network(X, y):\n",
    " learning_rate = 0.1\n",
    " W1 = np.random.rand(2, 2)\n",
    " W2 = np.random.rand(2, 1)\n",
    " for epoch in range(10000):\n",
    "   hidden = sigmoid(np.dot(X, W1))\n",
    " output = sigmoid(np.dot(hidden, W2))\n",
    " error = (y - output)\n",
    " delta2 = 2 * error * (output * (1 - output))\n",
    " delta1 = delta2.dot(W2.T) * (hidden * (1 - hidden))\n",
    " W2 += learning_rate * hidden.T.dot(delta2)\n",
    " W1 += learning_rate * X.T.dot(delta1)\n",
    " return {\n",
    " \"output\": np.round(output).flatten(),\n",
    " \"hidden\": hidden,\n",
    " \"W1\": W1, \"W2\": W2\n",
    " }\n",
    "from functools import partial\n",
    "def nn_predict(W1, W2, X):\n",
    " return sigmoid(np.dot(sigmoid(np.dot(X, W1)), W2))\n",
    "def get_df(X, y, preds):\n",
    " preds=np.array(preds)\n",
    " preds=preds.tolist()\n",
    " preds=sum(preds, [])\n",
    " preds=[round(x) for x in preds]\n",
    " df = pd.DataFrame(X, columns=['input1', 'input2'])\n",
    " df['actual'] = y\n",
    " df['predicted'] = preds\n",
    " return df.astype(int)\n",
    "np.random.seed(47)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "GATES = {\n",
    " \"OR\": [0, 1, 1, 1],\n",
    " \"AND\": [0, 0, 0, 1],\n",
    " \"XOR\": [0, 1, 1, 0],\n",
    " \"NOR\": [1, 0, 0, 0],\n",
    " \"NAND\": [1, 1, 1, 0],\n",
    " \"XNOR\": [1, 0, 0, 1],\n",
    "}\n",
    "for gate in GATES:\n",
    " y = np.array([GATES[gate]]).T\n",
    " result = neural_network(X, y)\n",
    " result = nn_predict(result['W1'],result['W2'],X)\n",
    " print(f'Truth Table of {gate}')\n",
    " print(get_df(X, GATES[gate], result))\n",
    " print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51868610",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 40 (330060551.py, line 41)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 41\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 40\n"
     ]
    }
   ],
   "source": [
    "# question image captioning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "images_path = '../input/flickr8k-sau/Flickr_Data/Images/'\n",
    "images = glob(images_path+'*.jpg')\n",
    "len(images)\n",
    "images[:5]\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    " plt.figure()\n",
    " img = cv2.imread(images[i])\n",
    " img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    " plt.imshow(img)\n",
    "from keras.applications import ResNet50\n",
    "incept_model = ResNet50(include_top=True)\n",
    "from keras.models import Model\n",
    "last = incept_model.layers[-2].output\n",
    "modele = Model(inputs = incept_model.input,outputs = last)\n",
    "modele.summary()\n",
    "images_features = {}\n",
    "count = 0\n",
    "for i in images:\n",
    " img = cv2.imread(i)\n",
    " img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    " img = cv2.resize(img, (224,224))\n",
    "\n",
    " img = img.reshape(1,224,224,3)\n",
    " pred = modele.predict(img).reshape(2048,)\n",
    "\n",
    " img_name = i.split('/')[-1]\n",
    "\n",
    " images_features[img_name] = pred\n",
    "\n",
    " count += 1\n",
    "\n",
    " if count > 1499:\n",
    " break\n",
    "\n",
    " elif count % 50 == 0:\n",
    " print(count)\n",
    "\n",
    "caption_path = '../input/flickr8ksau/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\n",
    "captions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')\n",
    "len(captions)\n",
    "captions_dict = {}\n",
    "for i in captions:\n",
    " try:\n",
    " img_name = i.split('\\t')[0][:-2]\n",
    " caption = i.split('\\t')[1]\n",
    " if img_name in images_features:\n",
    " if img_name not in captions_dict:\n",
    " captions_dict[img_name] = [caption]\n",
    "\n",
    " else:\n",
    " captions_dict[img_name].append(caption)\n",
    "\n",
    " except:\n",
    " pass\n",
    "len(captions_dict)\n",
    "VISUALIZE IMAGES\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(5):\n",
    " plt.figure()\n",
    " img_name = images[i]\n",
    "\n",
    "\n",
    " img = cv2.imread(img_name)\n",
    "\n",
    " img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    " plt.xlabel(captions_dict[img_name.split('/')[-1]])\n",
    " plt.imshow(img)\n",
    "import matplotlib.pyplot as plt\n",
    "for k in images_features.keys():\n",
    " plt.figure()\n",
    "\n",
    " img_name = '../input/flickr8k-sau/Flickr_Data/Images/' + k\n",
    "\n",
    "\n",
    " img = cv2.imread(img_name)\n",
    "\n",
    " img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    " plt.xlabel(captions_dict[img_name.split('/')[-1]])\n",
    " plt.imshow(img)\n",
    "\n",
    " break\n",
    "def preprocessed(txt):\n",
    " modified = txt.lower()\n",
    " modified = 'startofseq ' + modified + ' endofseq'\n",
    " return modified\n",
    "\n",
    "for k,v in captions_dict.items():\n",
    " for vv in v:\n",
    " captions_dict[k][v.index(vv)] = preprocessed(vv)\n",
    "CREATE VOCAL\n",
    "count_words = {}\n",
    "for k,vv in captions_dict.items():\n",
    " for v in vv:\n",
    " for word in v.split():\n",
    " if word not in count_words:\n",
    " count_words[word] = 0\n",
    " else:\n",
    " count_words[word] += 1\n",
    "len(count_words)\n",
    "THRESH = -1\n",
    "count = 1\n",
    "new_dict = {}\n",
    "for k,v in count_words.items():\n",
    " if count_words[k] > THRESH:\n",
    " new_dict[k] = count\n",
    " count += 1\n",
    "\n",
    "len(new_dict)\n",
    "new_dict['<OUT>'] = len(new_dict)\n",
    "captions_backup = captions_dict.copy()\n",
    "captions_dict = captions_backup.copy()\n",
    "for k, vv in captions_dict.items():\n",
    " for v in vv:\n",
    " encoded = []\n",
    " for word in v.split():\n",
    " if word not in new_dict:\n",
    " encoded.append(new_dict['<OUT>'])\n",
    " else:\n",
    " encoded.append(new_dict[word])\n",
    " captions_dict[k][vv.index(v)] = encoded\n",
    "captions_dict\n",
    "Batch_size = 5000\n",
    "VOCAB_SIZE = len(new_dict)\n",
    "Build GENERATOR\n",
    "def generator(photo, caption):\n",
    " n_samples = 0\n",
    "\n",
    " X = []\n",
    " y_in = []\n",
    " y_out = []\n",
    "\n",
    " for k, vv in caption.items():\n",
    " for v in vv:\n",
    " for i in range(1, len(v)):\n",
    " X.append(photo[k])\n",
    " in_seq= [v[:i]]\n",
    " out_seq = v[i]\n",
    " in_seq = pad_sequences(in_seq, maxlen=MAX_LEN,\n",
    "padding='post', truncating='post')[0]\n",
    " out_seq = to_categorical([out_seq],\n",
    "num_classes=VOCAB_SIZE)[0]\n",
    " y_in.append(in_seq)\n",
    " y_out.append(out_seq)\n",
    "\n",
    " return X, y_in, y_out\n",
    "X, y_in, y_out = generator(images_features, captions_dict)\n",
    "len(X), len(y_in), len(y_out)\n",
    "X = np.array(X)\n",
    "y_in = np.array(y_in, dtype='float64')\n",
    "y_out = np.array(y_out, dtype='float64')\n",
    "X.shape, y_in.shape, y_out.shape\n",
    "MODEL\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout,\n",
    "LSTM, TimeDistributed, Embedding, Bidirectional, Activation,\n",
    "RepeatVector,Concatenate\n",
    "from keras.models import Sequential, Model\n",
    "embedding_size = 128\n",
    "max_len = MAX_LEN\n",
    "vocab_size = len(new_dict)\n",
    "image_model = Sequential()\n",
    "image_model.add(Dense(embedding_size, input_shape=(2048,),\n",
    "activation='relu'))\n",
    "image_model.add(RepeatVector(max_len))\n",
    "image_model.summary()\n",
    "language_model = Sequential()\n",
    "language_model.add(Embedding(input_dim=vocab_size,\n",
    "output_dim=embedding_size, input_length=max_len))\n",
    "language_model.add(LSTM(256, return_sequences=True))\n",
    "language_model.add(TimeDistributed(Dense(embedding_size)))\n",
    "language_model.summary()\n",
    "conca = Concatenate()([image_model.output, language_model.output])\n",
    "x = LSTM(128, return_sequences=True)(conca)\n",
    "x = LSTM(512, return_sequences=False)(x)\n",
    "x = Dense(vocab_size)(x)\n",
    "out = Activation('softmax')(x)\n",
    "model = Model(inputs=[image_model.input, language_model.input], outputs =\n",
    "out)\n",
    "# model.load_weights(\"../input/model_weights.h5\")\n",
    "model.compile(loss='categorical_crossentropy', optimizer='RMSprop',\n",
    "metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([X, y_in], y_out, batch_size=512, epochs=50)\n",
    "inv_dict = {v:k for k, v in new_dict.items()}\n",
    "model.save('model.h5')\n",
    "model.save_weights('mine_model_weights.h5')\n",
    "np.save('vocab.npy', new_dict)\n",
    "def getImage(x):\n",
    "\n",
    " test_img_path = images[x]\n",
    " test_img = cv2.imread(test_img_path)\n",
    " test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    " test_img = cv2.resize(test_img, (299,299))\n",
    " test_img = np.reshape(test_img, (1,299,299,3))\n",
    "\n",
    " return test_img\n",
    "PREDICITON\n",
    "for i in range(5):\n",
    "\n",
    " no = np.random.randint(1500,7000,(1,1))[0,0]\n",
    " test_feature = modele.predict(getImage(no)).reshape(1,2048)\n",
    "\n",
    " test_img_path = images[no]\n",
    " test_img = cv2.imread(test_img_path)\n",
    " test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    " text_inp = ['startofseq']\n",
    " count = 0\n",
    " caption = ''\n",
    " while count < 25:\n",
    " count += 1\n",
    " encoded = []\n",
    " for i in text_inp:\n",
    " encoded.append(new_dict[i])\n",
    " encoded = [encoded]\n",
    " encoded = pad_sequences(encoded, padding='post', truncating='post',\n",
    "maxlen=MAX_LEN)\n",
    " prediction = np.argmax(model.predict([test_feature, encoded]))\n",
    " sampled_word = inv_dict[prediction]\n",
    " caption = caption + ' ' + sampled_word\n",
    "\n",
    " if sampled_word == 'endofseq':\n",
    " break\n",
    " text_inp.append(sampled_word)\n",
    "\n",
    " plt.figure()\n",
    " plt.imshow(test_img)\n",
    " plt.xlabel(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ee2d81",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 65 (2796041822.py, line 66)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 66\u001b[1;36m\u001b[0m\n\u001b[1;33m    for j in range(4):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 65\n"
     ]
    }
   ],
   "source": [
    "# question auto encoder and gan \n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten,\\\n",
    " Reshape, LeakyReLU as LR,\\\n",
    " Activation, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display # If using IPython, Colab or Jupyter\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "# Plot image data from x_train\n",
    "plt.imshow(x_train[0], cmap = \"gray\")\n",
    "plt.show()\n",
    "\n",
    "LATENT_SIZE = 32\n",
    "encoder = Sequential([\n",
    " Flatten(input_shape = (28, 28)),\n",
    " Dense(512),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(256),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(128),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(64),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(LATENT_SIZE, activation=\"sigmoid\"),\n",
    "])\n",
    "decoder = Sequential([\n",
    " Dense(64, input_shape = (LATENT_SIZE,)),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(128),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(256),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(512),\n",
    " LR(),\n",
    " Dropout(0.5),\n",
    " Dense(784),\n",
    " Activation(\"sigmoid\"),\n",
    " Reshape((28, 28))\n",
    "])\n",
    "img = Input(shape = (28, 28))\n",
    "latent_vector = encoder(img)\n",
    "output = decoder(latent_vector)\n",
    "model = Model(inputs = img, outputs = output)\n",
    "model.compile(\"nadam\", loss = \"binary_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()\n",
    "EPOCHS = 10\n",
    "#EPOCHS = 1000\n",
    "for epoch in range(EPOCHS):\n",
    " print(\"-----------\", \"EPOCH\", epoch, \"-----------\")\n",
    " fig, axs = plt.subplots(4, 4, figsize=(4,4))\n",
    " rand = x_test[np.random.randint(0, 10000, 16)].reshape((4, 4, 1, 28,\n",
    "28))\n",
    " #display.clear_output()\n",
    " for i in range(4):\n",
    " for j in range(4):\n",
    " axs[i, j].imshow(model.predict(rand[i, j])[0], cmap = \"gray\")\n",
    " axs[i, j].axis(\"off\")\n",
    " plt.subplots_adjust(wspace = 0, hspace = 0)\n",
    " plt.show()\n",
    " model.fit(x_train, x_train, batch_size = 64)\n",
    " print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3110f609",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 97) (4004838086.py, line 97)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 97\u001b[1;36m\u001b[0m\n\u001b[1;33m    print('discriminator: loss_real={},\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 97)\n"
     ]
    }
   ],
   "source": [
    "# gan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import *\n",
    "from keras.losses import *\n",
    "from keras.optimizers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')\n",
    "X = train_df.drop(['label'], axis=1).values.reshape(-1, 28,28) / 255\n",
    "y = train_df['label'].values\n",
    "X_test = test_df.values.reshape(-1, 28,28) / 255\n",
    "latent_input = Input(shape=(100,), name='latent_input')\n",
    "label_input = Input(shape=(1,), name='label_input')\n",
    "x = Embedding(input_dim=10, output_dim=10)(label_input)\n",
    "x = Reshape((10,))(x)\n",
    "x = concatenate([x, latent_input], axis=-1)\n",
    "x = Dense(7*7*128)(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Reshape((7,7,128))(x)\n",
    "x = UpSampling2D()(x)\n",
    "x = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = UpSampling2D()(x)\n",
    "x = Conv2D(32, kernel_size=3, strides=1, padding='same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(1, kernel_size=3, strides=1, padding='same')(x)\n",
    "x = Activation('sigmoid')(x)\n",
    "x = Reshape((28,28,))(x)\n",
    "generator = Model(inputs=[latent_input, label_input], outputs=x)\n",
    "generator.summary()\n",
    "img_input = Input(shape=(28,28,))\n",
    "x = Reshape((28,28,1))(img_input)\n",
    "x = Conv2D(16, kernel_size=3, strides=2, padding='same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(32, kernel_size=3, strides=2, padding='same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(11, activation='softmax')(x)\n",
    "discriminator = Model(inputs=img_input, outputs=x)\n",
    "discriminator.summary()\n",
    "latent_input = Input(shape=(100,), name='latent_input')\n",
    "label_input = Input(shape=(1,), name='label_input')\n",
    "x = generator([latent_input, label_input])\n",
    "x = discriminator(x)\n",
    "gan = Model(inputs=[latent_input, label_input], outputs=x)\n",
    "discriminator.trainable = False\n",
    "gan.summary()\n",
    "def random_mnist_set(X, y, size=10):\n",
    " length = len(X)\n",
    " indices = np.random.choice(length, size)\n",
    " return X[indices], y[indices]\n",
    "def random_generated_set(generator, size=10):\n",
    " latents = np.random.normal(0, 1, (size, 100))\n",
    " return generator.predict_on_batch({'latent_input':latents,\n",
    " 'label_input':np.random.randint(10, size=size)}\n",
    "save_interval = 300\n",
    "epochs = save_interval*10+1\n",
    "batch_size=20\n",
    "history = {'dloss':[], 'gloss':[]}\n",
    "for iteration in range(epochs):\n",
    " discriminator.trainable=True\n",
    " X_sample, y_sample = random_mnist_set(X, y, batch_size)\n",
    " dloss_real = discriminator.train_on_batch(X_sample, y_sample)\n",
    " dloss_fake =\n",
    "discriminator.train_on_batch(random_generated_set(generator,\n",
    "size=batch_size), np.full(batch_size, 10))\n",
    "\n",
    " discriminator.trainable=False\n",
    "\n",
    " gan_labels = np.random.randint(10, size=batch_size)\n",
    " gloss = gan.train_on_batch({'latent_input':np.random.normal(0, 1,\n",
    "(batch_size, 100)),\n",
    " 'label_input':gan_labels},\n",
    " gan_labels)\n",
    "\n",
    " history['dloss'].append((dloss_real[0]+dloss_fake[0])/2)\n",
    " history['gloss'].append(gloss[0])\n",
    " if iteration % save_interval == 0:\n",
    " print('generator: loss={}. acc={}'.format(gloss[0], gloss[1]))\n",
    " print('discriminator: loss_real={},\n",
    "acc_real={}'.format(dloss_real[0],dloss_real[1]))\n",
    " print(' loss_fake={},\n",
    "acc_fake={}'.format(dloss_fake[0],dloss_fake[1]))\n",
    " generator.save_weights('generator_{0:05d}.h5'.format(iteration))\n",
    "plt.plot(history['dloss'], 'r')\n",
    "plt.plot(history['gloss'], 'b')\n",
    "checkpoints = sorted(glob.glob('generator_*.h5'))\n",
    "plt.figure(figsize=(10,2*len(checkpoints)))\n",
    "for i,cp in enumerate(checkpoints):\n",
    " generator.load_weights(cp)\n",
    " generated =\n",
    "generator.predict_on_batch({'latent_input':np.random.normal(0, 1, (10,\n",
    "100)),\n",
    " 'label_input':np.arange(10)})\n",
    " for j, g in enumerate(generated):\n",
    " plt.subplot(len(checkpoints),10,10*i+j+1)\n",
    " plt.imshow(g, cmap='gray')\n",
    " plt.axis('off')\n",
    "generator.load_weights(checkpoints[-1])\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(10):\n",
    " generated =\n",
    "generator.predict_on_batch({'latent_input':np.random.normal(0, 1, (10,\n",
    "100)),\n",
    " 'label_input':np.arange(10)})\n",
    " for j, g in enumerate(generated):\n",
    " plt.subplot(len(checkpoints),10,10*i+j+1)\n",
    " plt.imshow(g, cmap='gray')\n",
    " plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a49acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights:  [array([[1.1272662]], dtype=float32), array([1.4571841], dtype=float32)]\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "Regression predictions:\n",
      "[[2.1335437]\n",
      " [2.358997 ]]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Regression predictions:\n",
      "[[2.2174046]\n",
      " [2.5998123]]\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Regression predictions:\n",
      "[[2.212587]\n",
      " [2.591891]]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prices-split-adjusted.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 137\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# In[13]:\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m prices_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprices-split-adjusted.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    138\u001b[0m fundamentals_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfundamentals.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# In[14]:\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Merge relevant columns from fundamentals into the prices dataframe\u001b[39;00m\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prices-split-adjusted.csv'"
     ]
    }
   ],
   "source": [
    "##build a deep neural network model start with linear regression using single variab\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)  # Input feature\n",
    "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)  # True labels with some noise\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,)))  # Single dense layer (linear regression)\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# Print the learned weights\n",
    "print(\"Learned weights: \", model.layers[0].get_weights())\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "new_X_reg = np.array([[0.6], [0.8]])\n",
    "predictions_reg = model.predict(new_X_reg)\n",
    "print(\"Regression predictions:\")\n",
    "print(predictions_reg)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "# Build a deep neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(1,), activation='relu'))  # Hidden layer 1\n",
    "model.add(Dense(16, activation='relu'))                   # Hidden layer 2\n",
    "model.add(Dense(1))                                       # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "new_X_reg = np.array([[0.6], [0.8]])\n",
    "predictions_reg = model.predict(new_X_reg)\n",
    "print(\"Regression predictions:\")\n",
    "print(predictions_reg)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "# Build a deep neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_shape=(1,), activation='relu'))  # Hidden layer 1\n",
    "model.add(Dense(16, activation='relu'))                   # Hidden layer 2\n",
    "model.add(Dense(1))                                       # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "new_X_reg = np.array([[0.6], [0.8]])\n",
    "predictions_reg = model.predict(new_X_reg)\n",
    "print(\"Regression predictions:\")\n",
    "print(predictions_reg)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# Load the data\n",
    "prices_df = pd.read_csv('prices-split-adjusted.csv')\n",
    "fundamentals_df = pd.read_csv('fundamentals.csv')\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "# Merge relevant columns from fundamentals into the prices dataframe\n",
    "data_df = pd.merge(prices_df, fundamentals_df[['Ticker Symbol']], how='inner', left_on='symbol', right_on='Ticker Symbol')\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# Choose the features and target\n",
    "X = data_df[['open', 'low', 'high']].values\n",
    "y = data_df['close'].values\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "# Build a neural network regression model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Mean Squared Error on Test Set: {loss:.4f}\")\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Print some example predictions and actual values\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {y_pred[i][0]:.4f}  |  Actual: {y_test[i]:.4f}\")\n",
    "\n",
    "# Calculate Mean Squared Error for the predictions\n",
    "mse = np.mean((y_pred - y_test) **2)\n",
    "print(f\"\\nMean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7b6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SpeechRecognition\n",
      "  Obtaining dependency information for SpeechRecognition from https://files.pythonhosted.org/packages/73/8c/74d3b2a7d71e3f18e1e50bf3f168cf3333846137f5723efac3d0dc5a8635/SpeechRecognition-3.10.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading SpeechRecognition-3.10.1-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in d:\\workspace\\anaconda\\lib\\site-packages (from SpeechRecognition) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\workspace\\anaconda\\lib\\site-packages (from SpeechRecognition) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspace\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\anaconda\\lib\\site-packages (from requests>=2.26.0->SpeechRecognition) (2023.7.22)\n",
      "Downloading SpeechRecognition-3.10.1-py2.py3-none-any.whl (32.8 MB)\n",
      "   ---------------------------------------- 0.0/32.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/32.8 MB 1.9 MB/s eta 0:00:18\n",
      "   ---------------------------------------- 0.1/32.8 MB 1.3 MB/s eta 0:00:26\n",
      "   ---------------------------------------- 0.2/32.8 MB 1.7 MB/s eta 0:00:19\n",
      "   ---------------------------------------- 0.3/32.8 MB 2.1 MB/s eta 0:00:16\n",
      "    --------------------------------------- 0.5/32.8 MB 2.4 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 0.8/32.8 MB 3.6 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 1.1/32.8 MB 3.8 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 1.7/32.8 MB 5.5 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 2.0/32.8 MB 6.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.0/32.8 MB 6.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.0/32.8 MB 6.2 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.7/32.8 MB 5.8 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 3.0/32.8 MB 5.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 3.0/32.8 MB 5.4 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 4.1/32.8 MB 6.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 4.1/32.8 MB 6.5 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 4.5/32.8 MB 6.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 4.5/32.8 MB 6.6 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 4.5/32.8 MB 6.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 5.1/32.8 MB 6.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 5.1/32.8 MB 6.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 5.6/32.8 MB 6.0 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 6.2/32.8 MB 6.3 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 6.3/32.8 MB 6.1 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 6.3/32.8 MB 6.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 6.9/32.8 MB 6.1 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 7.2/32.8 MB 6.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 7.4/32.8 MB 6.0 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 7.4/32.8 MB 6.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 7.5/32.8 MB 5.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 7.5/32.8 MB 5.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 7.5/32.8 MB 5.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 7.7/32.8 MB 5.4 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 7.8/32.8 MB 5.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 7.9/32.8 MB 5.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 8.0/32.8 MB 5.0 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 8.2/32.8 MB 5.0 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 8.3/32.8 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 8.5/32.8 MB 4.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 8.6/32.8 MB 4.8 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 8.8/32.8 MB 4.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 8.9/32.8 MB 4.7 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 9.0/32.8 MB 4.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 9.2/32.8 MB 4.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 9.3/32.8 MB 4.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 9.5/32.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 9.6/32.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 9.7/32.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 9.8/32.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 10.0/32.8 MB 4.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 10.2/32.8 MB 4.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 10.3/32.8 MB 4.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 10.5/32.8 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 10.6/32.8 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 10.8/32.8 MB 4.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 10.9/32.8 MB 4.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 11.0/32.8 MB 4.4 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 11.2/32.8 MB 4.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 11.3/32.8 MB 4.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 11.5/32.8 MB 4.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 11.6/32.8 MB 4.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 11.8/32.8 MB 4.1 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 12.0/32.8 MB 4.0 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 12.1/32.8 MB 3.9 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 12.3/32.8 MB 4.1 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 12.5/32.8 MB 4.0 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 12.6/32.8 MB 3.9 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 12.8/32.8 MB 3.9 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 12.9/32.8 MB 3.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 13.1/32.8 MB 3.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 13.2/32.8 MB 3.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 13.4/32.8 MB 3.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 13.5/32.8 MB 3.8 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 13.6/32.8 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 13.8/32.8 MB 3.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 13.9/32.8 MB 3.6 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.1/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.2/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.4/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.6/32.8 MB 3.4 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 14.7/32.8 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 14.8/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 15.0/32.8 MB 3.5 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 15.1/32.8 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 15.3/32.8 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 15.5/32.8 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 15.6/32.8 MB 3.4 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 15.8/32.8 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 15.9/32.8 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 16.0/32.8 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 16.2/32.8 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 16.3/32.8 MB 3.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 16.5/32.8 MB 3.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 16.6/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 16.8/32.8 MB 3.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 16.9/32.8 MB 3.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 17.1/32.8 MB 3.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 17.2/32.8 MB 3.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 17.4/32.8 MB 3.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 17.6/32.8 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 17.7/32.8 MB 3.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 17.9/32.8 MB 3.2 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 18.0/32.8 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 18.2/32.8 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 18.4/32.8 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 18.5/32.8 MB 3.2 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 18.7/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 18.8/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 18.9/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 19.1/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 19.3/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 19.4/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 19.6/32.8 MB 3.3 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 19.7/32.8 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 19.9/32.8 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 20.1/32.8 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 20.3/32.8 MB 3.3 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 20.4/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 20.6/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 20.8/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 20.9/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 21.1/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 21.2/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 21.4/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 21.5/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 21.7/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 21.9/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 22.0/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 22.2/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 22.3/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 22.5/32.8 MB 3.4 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 22.7/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 22.9/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.0/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.1/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.3/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.4/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.6/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 23.8/32.8 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.0/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.1/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.3/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 24.5/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 24.7/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 24.9/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 25.0/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 25.2/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 25.4/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 25.6/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 25.7/32.8 MB 3.5 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 25.9/32.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 26.1/32.8 MB 3.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 26.2/32.8 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.4/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.6/32.8 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 26.8/32.8 MB 3.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 27.0/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.2/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.3/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.5/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.7/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 27.8/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.0/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.1/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.3/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.5/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 28.6/32.8 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 28.8/32.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 29.0/32.8 MB 3.7 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 29.2/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 29.4/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 29.6/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 29.7/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 29.9/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 30.1/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 30.2/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.4/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.6/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.7/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 30.9/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.1/32.8 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.3/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.5/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.7/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 31.8/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 32.0/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.2/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.4/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.6/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.8/32.8 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 32.8/32.8 MB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudio\n",
      "  Obtaining dependency information for pyaudio from https://files.pythonhosted.org/packages/82/d8/f043c854aad450a76e476b0cf9cda1956419e1dacf1062eb9df3c0055abe/PyAudio-0.2.14-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading PyAudio-0.2.14-cp311-cp311-win_amd64.whl.metadata (2.7 kB)\n",
      "Downloading PyAudio-0.2.14-cp311-cp311-win_amd64.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.1 kB ? eta -:--:--\n",
      "   ---------------------------------------  163.8/164.1 kB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 164.1/164.1 kB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n",
      "Speak something:\n",
      "Sorry, could not understand the audio\n",
      "Collecting gTTS\n",
      "  Obtaining dependency information for gTTS from https://files.pythonhosted.org/packages/a7/ef/190f64a4edeb13165e3c598a08f06a2ae80cdae0aa208c96c20efdb7ad4b/gTTS-2.4.0-py3-none-any.whl.metadata\n",
      "  Downloading gTTS-2.4.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in d:\\workspace\\anaconda\\lib\\site-packages (from gTTS) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in d:\\workspace\\anaconda\\lib\\site-packages (from gTTS) (8.0.4)\n",
      "Requirement already satisfied: colorama in d:\\workspace\\anaconda\\lib\\site-packages (from click<8.2,>=7.1->gTTS) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\workspace\\anaconda\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\workspace\\anaconda\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\workspace\\anaconda\\lib\\site-packages (from requests<3,>=2.27->gTTS) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\workspace\\anaconda\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2023.7.22)\n",
      "Downloading gTTS-2.4.0-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: gTTS\n",
      "Successfully installed gTTS-2.4.0\n"
     ]
    }
   ],
   "source": [
    "#write a program to covert  a) sppech into text b) text into speech \n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "get_ipython().system('pip install SpeechRecognition')\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "get_ipython().system('pip install pyaudio')\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# Capture audio from microphone\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Speak something:\")\n",
    "    audio = recognizer.listen(source)\n",
    "# Recognize the speech\n",
    "try:\n",
    "    text = recognizer.recognize_google(audio)\n",
    "    print(\"You said:\", text)\n",
    "except sr.UnknownValueError:\n",
    "    print(\"Sorry, could not understand the audio\")\n",
    "except sr.RequestError as e:\n",
    "    print(f\"Error fetching results from Google Speech Recognition: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "#Converting Text to Speech:\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "get_ipython().system('pip install gTTS')\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "text = \"Hello, this is a text-to-speech example in deep learning .\"\n",
    "\n",
    "# Create a gTTS object\n",
    "tts = gTTS(text, lang='en')\n",
    "\n",
    "# Save the audio file\n",
    "tts.save(\"output.mp3\")\n",
    "\n",
    "# Play the audio file\n",
    "os.system(\"start output.mp3\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#C. Video into frames\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Open the video file\n",
    "video_path = 'F:\\\\Download July 2023\\\\model\\\\ED_sample.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# Get the frames per second (fps) and frame count\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# Loop through the frames and save each frame as an image\n",
    "for frame_num in range(frame_count):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        frame_filename = f'frame_{frame_num:04d}.jpg'\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        print(f\"Saved {frame_filename}\")\n",
    "    else:\n",
    "        print(f\"End of video at frame {frame_num}\")\n",
    "        break\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244ffb50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m fd\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# In[3]:\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#load dataset\u001b[39;00m\n\u001b[0;32m     13\u001b[0m (x_train,y_train),(x_test,y_test)\u001b[38;5;241m=\u001b[39mmnist\u001b[38;5;241m.\u001b[39mload_data()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fd' is not defined"
     ]
    }
   ],
   "source": [
    "# In[2]:\n",
    "\n",
    "#Number recognition usind MNIST dataset\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fd\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#load dataset\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#count the unique train labels\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train labels :\", dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#count the number of unique test labels\n",
    "unique, count = np.unique(y_test, return_counts=True)\n",
    "print(\"Test labels :\", dict(zip(unique,counts)))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#sample 25 mnist digit fromtrain dataset\n",
    "indexes =np.random.randint(0, x_train.shape[0], size =25)\n",
    "images=x_train[indexes]\n",
    "labels=y_train[indexes]\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#plot the 25 mnist digits\n",
    "plt.figure(figsize=(5,5))\n",
    "for i in range(len(indexes)):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    image=images[i]\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()\n",
    "plt.savefig(\"mnist-sample.png\")\n",
    "plt.close('all')\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#MNIST digit classifier model\n",
    "#compute the number of labels\n",
    "num_labels=len(np.unique(y_train))\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "# convert to one hot vector\n",
    "from keras.utils import to_categorical,plot_model\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "#image dimensions\n",
    "image_size=x_train.shape[1]\n",
    "input_size=image_size*image_size\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "#resize and normalize\n",
    "x_train= np.reshape(x_train,[-1,input_size])\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test =np.reshape(x_test,[-1,input_size])\n",
    "x_test=x_test.astype('float32')/255\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "#network parameter \n",
    "batch_size=128\n",
    "hidden_units=256\n",
    "dropout=0.45\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "#model ia s 3- layer MLP with Relu and dropout after each layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "model=Sequential()\n",
    "model.add(Dense(hidden_units,input_dim=input_size))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_labels))\n",
    "\n",
    "#this is the output for one hot encoder\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "plot_model(model,to_file='mlp_mnist.png',show_shapes=True)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#loss function for one hot vector\n",
    "#use of adam optimizer\n",
    "#accuracy is a good metrics for classification tasks\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "#train the network\n",
    "model.fit(x_train,y_train,epochs=20,batch_size=batch_size)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#validate the model on test dataset\n",
    "loss,acc=model.evaluate(x_test, y_test,batch_size=batch_size)\n",
    "print(\"\\nTest accuracy:%1f%%\" %(100*acc))\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baf20eae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint, ReduceLROnPlateau\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# In[20]:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Startiong variables\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# x_train = pd.read_csv('/kaggle/input/emnist/emnist-letters-train.csv')\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m x_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSKIT\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDeep Learning\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mLab\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mData set\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mEMNIST\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124memnist-letters-train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# In[21]:\u001b[39;00m\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-train.csv'"
     ]
    }
   ],
   "source": [
    "# char recog using rnn cnn\n",
    "# Imports \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "# Startiong variables\n",
    "# x_train = pd.read_csv('/kaggle/input/emnist/emnist-letters-train.csv')\n",
    "x_train = pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-train.csv')\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "x_train.head()\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "y_train = x_train['23']\n",
    "del x_train['7']\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# Highlighting the right answers\n",
    "y_train = utils.to_categorical(y_train)\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "# Divide the array into two parts\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)  # Convert to 2D image format\n",
    "x_train = np.rot90(x_train, axes=(1, 2))\n",
    "x_train = np.flip(x_train, axis=1)\n",
    "x_train = x_train.astype(np.float32)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "# Normalize for training (from 0 to 1)\n",
    "x_train /= 255.0\n",
    "\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "# We divide the data into two sets, the first for training and the second for verification\n",
    "    # test_size - percentage of the set for verification (10%)\n",
    "    # X_train - training kit (needs to be increased)\n",
    "    # X_val - verification dataset\n",
    "    # Y_train - the right answers for learning\n",
    "    # Y_val - correct answers to check\n",
    "random_seed = 2\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=random_seed)\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "# Expanding the data\n",
    "# Create a generator that rotates, zooms, and shifts images\n",
    "datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "# Create a neural network\n",
    "model = Sequential()\n",
    "# The first block consists of two layer\n",
    "# Subsample layer with selection of the maximum value in a 2 by 2# The output is a fully connected layer with 27 neurons (27 is the number of letters) squares of convolution, \n",
    "# each with 32 feature cards, the size of the convolution core is 2 by 2\n",
    "model.add(Conv2D(filters=64, kernel_size=(2, 2), padding='Same', activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(filters=64, kernel_size=(2, 2), padding='Same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# A layer that shuts down neurons with a 25 percent probability to prevent overlearning\n",
    "model.add(Dropout(0.25))\n",
    "# The second convolution block - two layers of convolution, \n",
    "# each with 32 feature cards, the size of the convolution core is 2 by 2\n",
    "model.add(Conv2D(filters=64, kernel_size=(2, 2), padding='Same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=(2, 2), padding='Same', activation='relu'))\n",
    "# Subsample layer with selection of the maximum value in a 2 by 2 square\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# A layer that shuts down neurons with a 25 percent probability to prevent overlearning\n",
    "model.add(Dropout(0.25))\n",
    "# The end of the convolutional part of the neural networkmodel.add(Dense(27, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "# The fully connected part, responsible for classification\n",
    "# The layer converts the two-dimensional output of the folding part into a one-dimensional array\n",
    "model.add(Flatten())\n",
    "# A fully connected layer with 256 neurons\n",
    "model.add(Dense(256, activation='relu'))\n",
    "# A layer that shuts down neurons with a 25 percent probability to prevent overlearning\n",
    "model.add(Dropout(0.25))\n",
    "# The output is a fully connected layer with 27 neurons (27 is the number of letters)\n",
    "model.add(Dense(27, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "# Compile the neural network\n",
    "# Loss function - categorical cross-entropy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "# Mini-sample size - 96 images\n",
    "batch_size = 96\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "# Network training\n",
    "# We use two flasks\n",
    "# The first colbel is responsible for saving the best network option\n",
    "# val_acc - the proportion of correct answers on the test set, save only the best network to a file named mnist_cnn.hdf5\n",
    "chekpoint = ModelCheckpoint('CNN_model_3_EMNIST_best.hdf5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "# The second callback is changing the speed of learning\n",
    "# If the quality of the set under test does not change in three iterations, the speed parameter is divided in half\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, factor=0.5,\n",
    "                                                min_lr=0.00001)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# For training, we use the generator we created earlier\n",
    "history = model.fit(datagen.flow(X_train, Y_train, batch_size=batch_size), epochs=50,\n",
    "                    validation_data=(X_val, Y_val), steps_per_epoch=X_train.shape[0] // batch_size, verbose=1,\n",
    "                    callbacks=[chekpoint, learning_rate_reduction])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Display the graph of the learning outcome\n",
    "history_dict = history.history\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "class_mapping = ' ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "import random\n",
    "\n",
    "testing_letter = pd.read_csv('/kaggle/input/emnist/emnist-letters-test.csv')\n",
    "\n",
    "x2 = np.array(testing_letter.iloc[:,1:].values)\n",
    "test_images = x2 / 255.0\n",
    "\n",
    "test_images_number = test_images.shape[0]\n",
    "test_images_height = 28\n",
    "test_images_width = 28\n",
    "test_images_size = test_images_height*test_images_width\n",
    "\n",
    "test_images = test_images.reshape(test_images_number, test_images_height, test_images_width, 1)\n",
    "test_images = np.rot90(test_images, axes=(1, 2))\n",
    "test_images = np.flip(test_images, axis=1)\n",
    "right = 0\n",
    "wrong = 0\n",
    "for idx in range(1,1000):\n",
    "    # idx = random.randint(0, 8000)\n",
    "    result = np.argmax(model.predict(test_images[idx:idx+1]))\n",
    "    answer = testing_letter.values[idx,0]\n",
    "    if (result == answer):\n",
    "        right += 1\n",
    "    else:\n",
    "        wrong += 1\n",
    "print(\"Correct answer = \", right, \" / 1 000\")\n",
    "print(\"Incorrect answer = \", wrong, \" / 1 000\")\n",
    "\n",
    "    # print('Prediction: ', result, ', Char: ', class_mapping[result])\n",
    "    # print('Label: ', testing_letter.values[idx,0])\n",
    "    # show_img(testing_letter, idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3ca8c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     14\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install emnist\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# In[3]:\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#load mnist dataset\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#(x_train, y_train),(x_test, y_test)=emnist.load_data()\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m x\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSKIT\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDeep Learning\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mLab\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mData set\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mEMNIST\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124memnist-letters-train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m x1\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDELL\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSKIT\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDeep Learning\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mLab\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mData set\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mEMNIST\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124memnist-letters-test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# In[4]:\u001b[39;00m\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mD:\\workspace\\anaconda\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-train.csv'"
     ]
    }
   ],
   "source": [
    "#write aprogram for character recognition using b)RNN\n",
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation ,Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.datasets import mnist\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "get_ipython().system('pip install emnist')\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#load mnist dataset\n",
    "#(x_train, y_train),(x_test, y_test)=emnist.load_data()\n",
    "x=pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-train.csv')\n",
    "x1= pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-test.csv')\n",
    "                                                                                                    \n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "x.head()\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "y_train=x['1']\n",
    "x_train= x.drop(columns=['1'])\n",
    "#del x_train['7']\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "#compute the number of labels\n",
    "num_labels=len(np.unique(y_train))\n",
    "print(num_labels)\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "x1.head()\n",
    "\n",
    "\n",
    "# In[116]:\n",
    "\n",
    "\n",
    "y_test=x1['1']\n",
    "x_train= x1.drop(columns=['1'])\n",
    "#del x_train['7']\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "\n",
    "# In[117]:\n",
    "\n",
    "\n",
    "#compute the number of labels\n",
    "num_labels=len(np.unique(y_test))\n",
    "print(num_labels)\n",
    "\n",
    "\n",
    "# In[126]:\n",
    "\n",
    "\n",
    "num_classes = len(np.unique(y_test))\n",
    "y_test = to_categorical(y_test, num_classes=num_classes+1)\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = to_categorical(y_train, num_classes=num_classes+1)\n",
    "\n",
    "\n",
    "# In[132]:\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(-1, 132, 785)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, input_shape=(sequence_length, num_features), return_sequences=True))\n",
    "model.add(SimpleRNN(128, return_sequences=False))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# In[128]:\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# In[131]:\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "\n",
    "# In[54]:\n",
    "\n",
    "\n",
    "#input image dimensions\n",
    "image_size =x_train.shape[1]\n",
    "#resize and normalize\n",
    "x_train= np.reshape(x_train,[-1, image_size, image_size, 1])\n",
    "x_test=np.reshape(x_test, [-1,image_size,image_size,1])\n",
    "x_train=x_train.astype('float32')/255\n",
    "x_test=x_test.astype('float32')/255\n",
    "\n",
    "\n",
    "# In[82]:\n",
    "\n",
    "\n",
    "#image is process as is squared grayscale\n",
    "input_shape =(image_size, image_size, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "pool_size =2\n",
    "filters =64\n",
    "dropout = 0.2\n",
    "num_features=785\n",
    "sequence_length=132\n",
    "\n",
    "\n",
    "# In[89]:\n",
    "\n",
    "\n",
    "#model is stackof CNN-ReluMaxPooling\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, TimeDistributed, Flatten\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, input_shape=(sequence_length, num_features), return_sequences=True))\n",
    "model.add(SimpleRNN(128, return_sequences=False))  # You may add more layers as needed\n",
    "model.add(Dense(26, activation='sigmoid'))  # Output layer for character recognition\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[90]:\n",
    "\n",
    "\n",
    "#loss function for one hot vector\n",
    "# use of adam optimizer\n",
    "#accuracy \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# In[91]:\n",
    "\n",
    "\n",
    "#train themodel\n",
    "#model.fit(x_train, y_train, epochs=10, batch_size=batch_size)\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=128, validation_split=0.2)\n",
    "\n",
    "\n",
    "# In[61]:\n",
    "\n",
    "\n",
    "loss,acc =model.evaluate(x_test, y_test, batch_size = batch_size)\n",
    "print(\"\\n Test Accuracy :%.1f%%\" % (100.0*acc))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "# Load and preprocess the custom image\n",
    "\n",
    "#custom_image_path = 'C:\\\\Users\\\\DELL\\\\Downloads\\\\6.png'\n",
    "custom_image_path = 'C:\\\\Users\\\\DELL\\\\Downloads\\\\MNIST_6_0.jpeg'\n",
    "\n",
    "\n",
    "custom_image = image.load_img(custom_image_path, target_size=(28, 28), color_mode='grayscale')\n",
    "custom_image = image.img_to_array(custom_image)\n",
    "custom_image = custom_image.astype('float32') / 255.0\n",
    "custom_image = np.expand_dims(custom_image, axis=0)  # Add a batch dimension\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image = mpimg.imread(custom_image_path)\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "#plt.axis('off')  # Turn off axis labels and ticks\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "# Make predictions for the custom image\n",
    "predictions = model.predict(custom_image)\n",
    "#print(predictions)\n",
    "# The 'predictions' variable now contains the predicted probabilities for each class (0-9).\n",
    "# You can find the predicted class (character or digit) with the highest probability as follows:\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Display the predicted class\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image = mpimg.imread('C:\\\\Users\\\\DELL\\\\Downloads\\\\MNIST_6_0.jpeg')\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis labels and ticks\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Load the EMNIST data (assuming it's in CSV format)\n",
    "# You may need to adjust the path and column names based on your dataset.\n",
    "df = pd.read_csv('C:\\\\Users\\\\DELL\\\\OneDrive\\\\Desktop\\\\SKIT\\\\Deep Learning\\\\Lab\\\\Data set\\\\EMNIST\\\\emnist-letters-train.csv')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "# Extract features (X) and labels (y) from the DataFrame\n",
    "X = df.drop(columns=['1']).values\n",
    "y = df['1'].values\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "print(X_train.shape[1])\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "# Define the RNN model= 784\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, input_shape=(X_train.shape[1], 1), return_sequences=True))  # Single feature per time step\n",
    "model.add(SimpleRNN(128, return_sequences=False))  # You may add more layers as needed\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Output layer for character recognition\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Load the image using OpenCV (assuming it's a grayscale image)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Check if the image is loaded successfully\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Failed to load image from {image_path}\")\n",
    "    \n",
    "    # Resize the image to match the expected input size of the model\n",
    "    # You may need to adjust the dimensions based on your model's input shape\n",
    "    resized_image = cv2.resize(image, (28, 28))\n",
    "    \n",
    "    # Normalize the pixel values to the range [0, 1]\n",
    "    normalized_image = resized_image / 255.0\n",
    "    \n",
    "    # Ensure that the image has the correct data type (float32)\n",
    "    preprocessed_image = np.float32(normalized_image)\n",
    "    \n",
    "    return preprocessed_image\n",
    "\n",
    "\n",
    "# ##### image_path = 'image_to_predict.png'  # Replace with the path to your image\n",
    "# preprocessed_image = load_and_preprocess_image(image_path)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Reshape the image data to match the model's input shape\n",
    "\n",
    "image = image.reshape(1, sequence_length, 1)  # 1 example, sequence_length, 1 feature per pixel\n",
    "\n",
    "# Make predictions on the image\n",
    "predictions = model.predict(image)\n",
    "\n",
    "# Get the predicted character label\n",
    "predicted_label = np.argmax(predictions)\n",
    "\n",
    "# Decode the predicted label if you've used label encoding\n",
    "decoded_label = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "print(f\"Predicted Label: {decoded_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524bdb27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
